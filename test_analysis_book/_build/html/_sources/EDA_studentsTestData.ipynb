{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analyis of MasteryConnect Student Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Student Data analysis](../notebook/images/Info5-980x604.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "Teachers make on average, around 1500 decisions per day [link](https://www.continentalpress.com/blog/tips-for-reducing-teacher-decision-fatigue/#:~:text=According%20to%20data%20from%20busyteacher,just%20during%20the%20school%20day.) This may seem like a particularly dramatic statistic to some people outside the profession, but it is an enduring reality for those teachers who give their all in pursuit of positive academic outcomes. \n",
    "Tracking and monitoring student performance is crucial to acheiving this, however, manually doing so throughout the year, is not only challenging, but quite difficult to sustain. \n",
    "This project aims to alleviate this burden by:\n",
    "\n",
    "- Collecting CSV assessment data from Mastery Connect\n",
    "- Processing and uploading datasets to MongoDB\n",
    "- Generating targeted insights using MongoDB's aggregation pipelines and pandas (a Python data manipulation library)\n",
    "- Presenting these insights through intuitive visualizations\n",
    "\n",
    "The key performance indicators generated by this system, such as standard mastery rates, question type proficiency, and individual student growth trajectories are invaluable insights that will empower teachers to implement data-driven classroom strategies, help students understand their progress, and allow administrators to identify broader trends.\n",
    "\n",
    "Designed with scalability in mind, the system can potentially extend to multiple classrooms or entire school districts, significantly reducing decision fatigue for educators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Analysis Topic                    | Question                                                                 | Approach                                                                                                           |\n",
    "|----------------------------------|--------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------|\n",
    "| Overall Performance Analysis      | What is the overall distribution of student performance across all tests? | Analyze descriptive statistics of overall scores and visualize the distribution.                                   |\n",
    "| Standard-Based Performance Analysis | How do students perform across different biology standards?               | a) Calculate and visualize average performance for each standard.<br>b) Identify top and bottom performing students for each standard.<br>c) Rank standards by difficulty based on average performance. |\n",
    "| Question Type Analysis            | How does performance vary across different question types?               | Calculate and visualize difficulty rankings for different question types.                                           |\n",
    "| Performance Analysis Over Time    | How has student performance changed over time?                           | Analyze and visualize performance trends over time for all students.                                                |\n",
    "| Test Participation Analysis       | Are there patterns in test participation or missed tests?                 | Identify and analyze data for students who did not complete certain tests.                                          |\n",
    "| Depth of Knowledge (DOK) Analysis | How does performance correlate with the depth of knowledge required?     | Analyze performance across different DOK levels.                                                                    |\n",
    "| Class and Teacher Impact Analysis | Are there significant performance differences across classes or teachers? | Compare performance metrics across different classes and teachers.                                                  |\n",
    "| Standard Coverage Analysis        | Which standards are most frequently tested, and how does this relate to performance? | Analyze the frequency of questions for each standard and correlate with performance data.                            |\n",
    "| Individual Student Progress Tracking | How can we visualize and analyze individual student progress over time?    | Develop interactive visualizations for tracking individual student performance across tests and standards.        |\n",
    "| Test Design Analysis              | Are there patterns in test composition that correlate with overall performance? | Analyze the relationship between test characteristics (e.g., question types, standards covered) and overall performance. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from src.components.db_connection import get_database, build_query \n",
    "from src.pipelines import mongo_db_pipelines as m_db\n",
    "from src.components.data_ingestion import drop_collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = get_database(create_indices=True)\n",
    "students = db['students']\n",
    "tests = db['tests']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter data for particular teacher(s) or time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "teacher = os.getenv('TEACHER_2')\n",
    "class_name = ''\n",
    "start_date = ''\n",
    "\n",
    "query = build_query(teacher=teacher, class_name=class_name, start_date=start_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical analysis \n",
    "#### numeric data across all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import describe_numeric_field_pandas\n",
    "# Average scores on tests\n",
    "overall_scores_stats = describe_numeric_field_pandas(students, 'overall_percentage', query)\n",
    "\n",
    "\n",
    "display(pd.DataFrame(overall_scores_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency distribution across all classes\n",
    "\n",
    "This shows the distribution of fields (count values) by collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_frequency_distribution\n",
    "\n",
    "# MongoDB pipeline for frequency distribution\n",
    "standard_dist = get_frequency_distribution(db=db, field='questions', subfield='standard', teacher=teacher)\n",
    "item_type_dist = get_frequency_distribution(db=db, field='questions', subfield='item_type_name', teacher=teacher)\n",
    "dok_dist = get_frequency_distribution(db=db, field='questions', subfield='dok', teacher=teacher)\n",
    "\n",
    "# Renaming primary key \n",
    "standard_dist = pd.DataFrame(standard_dist).rename(columns={'_id':'standards'})\n",
    "item_type_dist = pd.DataFrame(item_type_dist).rename(columns={'_id':'item_type'})\n",
    "dok_dist = pd.DataFrame(dok_dist).rename(columns={'_id':'dok'})\n",
    "\n",
    "print(\"Frequency of Course Standards\")\n",
    "display(pd.DataFrame(standard_dist).T)\n",
    "print(\"Distribution of Question types\")\n",
    "display(pd.DataFrame(item_type_dist))\n",
    "print(\"Depth of knowledge Frequency\")\n",
    "display(pd.DataFrame(dok_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution Visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import create_frequency_distributions_plot\n",
    "fig_distributions = create_frequency_distributions_plot(standard_dist, item_type_dist, dok_dist)\n",
    "display(fig_distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance over time for all students \n",
    "\n",
    "Aggreation of test results by date, not including students who missed a test (NaN values filtered out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import performance_trend\n",
    "\n",
    "# Run the performance trend analysis\n",
    "performance_trend_df = pd.DataFrame(performance_trend(students, query))\n",
    "print('First 5 tests by date and average')\n",
    "display(performance_trend_df.head(5))\n",
    "\n",
    "print('Last 5 tests by date and average')\n",
    "display(performance_trend_df.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vizualization for performance trend overtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import create_performance_trend_plot\n",
    "fig_performance = create_performance_trend_plot(performance_trend_df)\n",
    "display(fig_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Students who did not complete a test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import identify_students_with_nan, check_responses_for_nan\n",
    "\n",
    "missed_test = identify_students_with_nan(students, 'overall_percentage')\n",
    "\n",
    "# convert to data frame\n",
    "missed_test_df = pd.DataFrame(missed_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anonymizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from faker import Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Custom function to generate fake test IDs\n",
    "def fake_test_id():\n",
    "    class_names = ['Biology', 'Chemistry', 'Anatomy']\n",
    "    test_names = ['Midterm', 'Final', 'Quiz', 'Project', 'Exam']\n",
    "    \n",
    "    class_name = random.choice(class_names)\n",
    "    test_name = random.choice(test_names)\n",
    "    fake_date = fake.date_between(start_date='-1y', end_date='today').strftime('%Y%m%d')\n",
    "    \n",
    "    return f\"{class_name}_{test_name}_{fake_date}\"\n",
    "\n",
    "# Create mappings for each column\n",
    "def create_mapping(original_values, fake_function):\n",
    "    return {value: fake_function() for value in set(original_values)}\n",
    "\n",
    "student_id_mapping = create_mapping(missed_test_df['student_id'], lambda: f\"S-{fake.numerify('######')}\")\n",
    "first_name_mapping = create_mapping(missed_test_df['first_name'], fake.first_name)\n",
    "last_name_mapping = create_mapping(missed_test_df['last_name'], fake.last_name)\n",
    "teacher_mapping = create_mapping(missed_test_df['teacher'], fake.name)\n",
    "test_id_mapping = create_mapping(missed_test_df['test_id'], fake_test_id)\n",
    "\n",
    "# Apply mappings to create anonymized dataframe\n",
    "anonymized_df = missed_test_df.copy()\n",
    "anonymized_df['student_id'] = anonymized_df['student_id'].map(student_id_mapping)\n",
    "anonymized_df['first_name'] = anonymized_df['first_name'].map(first_name_mapping)\n",
    "anonymized_df['last_name'] = anonymized_df['last_name'].map(last_name_mapping)\n",
    "anonymized_df['teacher'] = anonymized_df['teacher'].map(teacher_mapping)\n",
    "anonymized_df['test_id'] = anonymized_df['test_id'].map(test_id_mapping)\n",
    "\n",
    "# Display the result\n",
    "display(anonymized_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addirional test stats\n",
    "\n",
    "What tests did each student miss? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import display_interactive_student_table\n",
    "\n",
    "display_interactive_student_table(anonymized_df, width='80%', height= '400px')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Performance by Standard:\n",
    "\t\tQuestion: How has each student performed per standard? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the pipeline\n",
    "pipeline = m_db.comprehensive_test_analysis_pipeline(query)\n",
    "results = list(students.aggregate(pipeline))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anonymizing Dataframe to protect sensitive data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def anonymize_df(df):\n",
    "    # Create a copy of the dataframe\n",
    "    anon_df = df.copy()\n",
    "    \n",
    "    # Create mappings for names and IDs\n",
    "    student_id_map = {id: hashlib.md5(str(id).encode()).hexdigest()[:8] for id in df['student_id'].unique()}\n",
    "    first_name_map = {name: fake.first_name() for name in df['first_name'].unique()}\n",
    "    last_name_map = {name: fake.last_name() for name in df['last_name'].unique()}\n",
    "    class_name_map = {class_name: f\"Fake-class{fake.numerify('####')}\" for class_name in df['class_name'].unique()}\n",
    "    \n",
    "    # Create a mapping for test_ids to ensure consistency\n",
    "    test_id_map = {}\n",
    "    for test_id in df['test_id'].unique():\n",
    "        parts = test_id.split('_')\n",
    "        if len(parts) >= 3:\n",
    "            new_test_id = f\"{class_name_map.get(parts[0], parts[0])}_{fake.word().capitalize()}_{fake.date_this_year().strftime('%Y%m%d')}\"\n",
    "        else:\n",
    "            new_test_id = f\"Test_{fake.word().capitalize()}_{fake.date_this_year().strftime('%Y%m%d')}\"\n",
    "        test_id_map[test_id] = new_test_id\n",
    "    \n",
    "    # Apply mappings\n",
    "    anon_df['student_id'] = anon_df['student_id'].map(student_id_map)\n",
    "    anon_df['first_name'] = anon_df['first_name'].map(first_name_map)\n",
    "    anon_df['last_name'] = anon_df['last_name'].map(last_name_map)\n",
    "    anon_df['class_name'] = anon_df['class_name'].map(class_name_map)\n",
    "    anon_df['test_id'] = anon_df['test_id'].map(test_id_map)\n",
    "    \n",
    "    # Update test_name based on the new test_id\n",
    "    anon_df['test_name'] = anon_df['test_id'].apply(lambda x: ' '.join(x.split('_')[1:-1]))\n",
    "    \n",
    "    \n",
    "    return anon_df\n",
    "\n",
    "# Apply the anonymization\n",
    "anonymized_df2 = anonymize_df(df)\n",
    "\n",
    "# Display the result\n",
    "display(anonymized_df2.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General test analysis data (note: anonymized df is being used instead of df)\n",
    "performance_by_standard = anonymized_df2.groupby(['student_id', 'first_name', 'last_name', 'standard', 'class_name']).agg({\n",
    "    'is_correct': ['mean', 'count']\n",
    "}).reset_index()\n",
    "performance_by_standard.columns = ['student_id', 'first_name', 'last_name', 'standard', 'class_name', 'avg_performance', 'total_questions']\n",
    "performance_by_standard['avg_performance'] *= 100  # Convert to percentage\n",
    "performance_by_standard['avg_performance'] = performance_by_standard['avg_performance'].round(2) # Round off averages\n",
    "\n",
    "display(performance_by_standard.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of performance by standards for all students of all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import create_performance_dash_app\n",
    "create_performance_dash_app(performance_by_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Performance by Standard:\n",
    "\t\tQuestion: Who are the top performing students for each standard? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Fake data (anonymized_df2 vs df)\n",
    "def get_top_n_students(anonymized_df2, n=5, upper=True):\n",
    "    # Create a rank within each standard\n",
    "    anonymized_df2['rank'] = anonymized_df2.groupby('standard')['avg_performance'].rank(method='first', ascending=not upper)\n",
    "    \n",
    "    # Filter to keep only the top/bottom n ranks\n",
    "    result = anonymized_df2[anonymized_df2['rank'] <= n]\n",
    "    \n",
    "    # Sort the final result to maintain the desired order\n",
    "    result = result.sort_values(['standard', 'avg_performance', 'total_questions'], \n",
    "                                ascending=[True, not upper, False])\n",
    "    \n",
    "    # Drop the temporary rank column\n",
    "    result = result.drop('rank', axis=1)\n",
    "    \n",
    "    return result.reset_index(drop=True)\n",
    "\n",
    "# Get top 5 students for each standard\n",
    "top_5_students = get_top_n_students(performance_by_standard, n=5, upper=True)\n",
    "print(\"Top 5 Performing Students for Each Standard (first four shown):\")\n",
    "display(top_5_students.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Performance by Standard:\n",
    "\t\tQuestion: Who are the lowest performing students for each standard? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lowest 5 students for each standard\n",
    "bottom_5_students = get_top_n_students(performance_by_standard, n=5, upper=False)\n",
    "\n",
    "print(\"Bottom 5 Performing Students for Each Standard (first four shown):\")\n",
    "display(bottom_5_students.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Performance by Standard:\n",
    "\t\tQuestion: Which standards are the most difficult as measured by average performance? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standards ranked by difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average performance for each standard (using the anonymized df instead of df)\n",
    "standard_difficulty = anonymized_df2.groupby('standard').agg({\n",
    "    'is_correct': ['mean', 'count'],\n",
    "    'student_id': 'nunique'\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "standard_difficulty.columns = ['standard', 'avg_performance', 'total_questions', 'unique_students']\n",
    "\n",
    "# Convert average performance to percentage and round\n",
    "standard_difficulty['avg_performance'] = (standard_difficulty['avg_performance'] * 100).round(2)\n",
    "\n",
    "# Sort by average performance (ascending) to see most difficult standards first\n",
    "standard_difficulty = standard_difficulty.sort_values('avg_performance', ascending=True)\n",
    "\n",
    "# Add a difficulty rank\n",
    "standard_difficulty['difficulty_rank'] = standard_difficulty['avg_performance'].rank(method='dense', ascending=True)\n",
    "\n",
    "# Reset index again\n",
    "standard_difficulty.reset_index(drop=True)\n",
    "\n",
    "print(\"Standards Ranked by Difficulty (Most Difficult First):\")\n",
    "display(standard_difficulty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Performance by Standard:\n",
    "\t\tQuestion: Which standards are students most proficient in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Highest scoring standards \n",
    "display(standard_difficulty.sort_values(by='difficulty_rank', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard performance overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import display_standards_difficulty_graph\n",
    "\n",
    "# Showing standards for chemistry as an example \n",
    "chm_stnds_diff = standard_difficulty[standard_difficulty['standard'].str.contains('Chm')]\n",
    "\n",
    "display_standards_difficulty_graph(chm_stnds_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Type Difficulty\n",
    "            Question: What are the difficulty rankings for different question types?\n",
    "           \n",
    "[What item types are in Mastery Connect?](https://community.canvaslms.com/t5/Mastery-Connect-Item-Authoring/What-item-types-can-I-create-in-Mastery-Connect/ta-p/562964)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate difficulty for each question type\n",
    "question_type_difficulty = anonymized_df2.groupby('question_type').agg({\n",
    "    'is_correct': ['mean', 'count'],\n",
    "    'student_id': 'nunique'\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "question_type_difficulty.columns = ['question_type', 'avg_performance', 'total_questions', 'unique_students']\n",
    "\n",
    "# Convert average performance to percentage and round\n",
    "question_type_difficulty['avg_performance'] = (question_type_difficulty['avg_performance'] * 100).round(2)\n",
    "\n",
    "# Sort by average performance (ascending) to see most difficult question types first\n",
    "question_type_difficulty = question_type_difficulty.sort_values('avg_performance', ascending=True)\n",
    "\n",
    "# Add a difficulty rank\n",
    "question_type_difficulty['difficulty_rank'] = question_type_difficulty['avg_performance'].rank(method='dense', ascending=True)\n",
    "\n",
    "print(\"Question Types Ranked by Difficulty (Most Difficult First):\")\n",
    "display(question_type_difficulty)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question type difficulty visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import display_question_type_difficulty_graph\n",
    "\n",
    "display_question_type_difficulty_graph(question_type_difficulty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Progress Overtime:\n",
    "\t\tQuestion: How has students progress changed over time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using anonymized df\n",
    "from src.utils import display_performance_figure\n",
    "# Check if we have a date column\n",
    "if 'date' not in anonymized_df2.columns:\n",
    "    # Extract date from test_id if necessary\n",
    "    anonymized_df2['date'] = pd.to_datetime(anonymized_df2['test_id'].str.split('_').str[-1], format='%Y%m%d')\n",
    "\n",
    "# Ensure date is in datetime format\n",
    "anonymized_df2['date'] = pd.to_datetime(anonymized_df2['date'])\n",
    "\n",
    "display_performance_figure(anonymized_df2, performance_trend_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06182396cdb8313612a974207f671ef2489ce3d9ce1a89cda8c406c5ed307752"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
