{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analyis of MasteryConnect Student Test Data\n",
    "\n",
    "## Adrian Allen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "I am examining this dataset to look for any actionable insights which could help teachers, students and all other stakeholders improve student performance. This is particularly valuable for courses with standardized tests such as Biology. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Objectives \n",
    "\n",
    "\t1.\tOverall Performance by Biology Standard:\n",
    "\t\tQuestion: Which biology standard has the best performance overall among groups?\n",
    "\t\tApproach: Calculate the average performance for each biology standard.\n",
    "\t2.\tTop Performing Students by Standards:\n",
    "\t\tQuestion: Who are the top-performing students for each biology standard?\n",
    "\t\tApproach: Identify students with the highest scores for each standard.\n",
    "\t3.\tLowest Performing Students by Standards:\n",
    "\t\tQuestion: Who are the lowest-performing students for each biology standard?\n",
    "\t\tApproach: Identify students with the lowest scores for each standard.\n",
    "\t4.\tQuestion Count by Standards:\n",
    "\t\tQuestion: Which standards have the most questions? Which have the least?\n",
    "\t\tApproach: Count the number of questions associated with each standard.\n",
    "\t5.\tWorst Performing Biology Standard:\n",
    "\t\tQuestion: Which biology standard had the worst performance?\n",
    "\t\tApproach: Calculate the average performance for each standard and identify the lowest.\n",
    "\t6.\tMost Missed Question for Each Test:\n",
    "\t\tQuestion: Which question was most frequently answered incorrectly for each test?\n",
    "\t\tApproach: Calculate the percentage of incorrect answers for each question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add the project root to sys.path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from src.components.data_ingestion import process_all_csv_files\n",
    "\n",
    "pd.set_option('display.max_columns', 30)\n",
    "\n",
    "load_dotenv()\n",
    "CSV_DIR = os.getenv('CSV_DIR')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import all csv files from Data folder and convert to dataframes\n",
    "dfs_test_info = process_all_csv_files(CSV_DIR)\n",
    "\n",
    "# Extract dataframe from tuple \n",
    "dfs = [df[0] for df in dfs_test_info]\n",
    "\n",
    "len(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Data into one Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data\n",
    "\n",
    "- null/na values\n",
    "- missing cells\n",
    "- mismatched values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create aggregate columns\n",
    "\n",
    "group by columns for average from each standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create total average for each student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of averages overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of Standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of Question types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### average over time\n",
    "\n",
    "is their a score trend by month or year? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### peformance by standard line graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### performance by question type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Performance by Biology Standard:\n",
    "\t\tQuestion: Which biology standard has the best performance overall among groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Performance by Biology Standard:\n",
    "\t\tQuestion: Which biology standard has the worst performance overall among groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\tTop Performing Students by Standards:\n",
    "\t\tQuestion: Who are the top-performing students for each biology standard?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowest Performing Students by Standards:\n",
    "\t\tQuestion: Who are the lowest-performing students for each biology standard?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Count by Standards:\n",
    "\t\tQuestion: Which standards have the most questions? Which have the least?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Missed Question for Each Test:\n",
    "\t\tQuestion: Which question was most frequently answered incorrectly for each test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06182396cdb8313612a974207f671ef2489ce3d9ce1a89cda8c406c5ed307752"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
